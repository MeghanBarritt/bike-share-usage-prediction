{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am manually copying the best parameters for each configuration as the `Hyperparameters` notebook takes a long time to run, and I want this to be more useable.<p>\n",
    "_(Note: XBGRegressor on my machine takes alpha and lambda as `reg_alpha` and `reg_lambda`, otherwise it doesn't recognize the variable.)_<p>\n",
    "_(Memo to me: the `X_train`s and `X_test`s change depending on what columns got dropped, but the target variable does not. It's always the same `y_train`s and `y_test`s.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned with all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DC set, all features parameters**:<br>\n",
    "alpha=0.1, lambda=100, learning_rate=0.3, max_depth=6, n_estimators=180, objective='reg:squarederror', random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dc = pd.read_pickle('pickles/split/X_train_dc.pkl')\n",
    "X_test_dc = pd.read_pickle('pickles/split/X_test_dc.pkl')\n",
    "y_train_dc = pd.read_pickle('pickles/split/y_train_dc.pkl')\n",
    "y_test_dc = pd.read_pickle('pickles/split/y_test_dc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_dc_allfeat = XGBRegressor(reg_alpha=0.1, reg_lambda=100, learning_rate=0.3, max_depth=6, n_estimators=180, objective='reg:squarederror', random_state=42)\n",
    "xgb_dc_allfeat.fit(X_train_dc,y_train_dc)\n",
    "\n",
    "dc_allfeat_pred_train = xgb_dc_allfeat.predict(X_train_dc)\n",
    "dc_allfeat_pred_test = xgb_dc_allfeat.predict(X_test_dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_allfeat_rmse_train = root_mean_squared_error(y_train_dc, dc_allfeat_pred_train)\n",
    "dc_allfeat_r2_train = r2_score(y_train_dc, dc_allfeat_pred_train)\n",
    "\n",
    "dc_allfeat_rmse_test = root_mean_squared_error(y_test_dc, dc_allfeat_pred_test)\n",
    "dc_allfeat_r2_test = r2_score(y_test_dc, dc_allfeat_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: \n",
      "Root Mean Squared Error: 27.238070723611994 \n",
      "R-Squared: 0.973456859588623\n",
      "Test: \n",
      "Root Mean Squared Error: 68.69202506023652 \n",
      "R-Squared: 0.9029864072799683\n"
     ]
    }
   ],
   "source": [
    "print(f'Train: \\nRoot Mean Squared Error: {dc_allfeat_rmse_train} \\nR-Squared: {dc_allfeat_r2_train}')\n",
    "print(f'Test: \\nRoot Mean Squared Error: {dc_allfeat_rmse_test} \\nR-Squared: {dc_allfeat_r2_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**London set, all features parameters**:<br>\n",
    "alpha=0.75, lambda=125, learning_rate=0.3, max_depth=7, n_estimators=70, objective='reg:squarederror', random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_l = pd.read_pickle('pickles/split/X_train_l.pkl')\n",
    "X_test_l = pd.read_pickle('pickles/split/X_test_l.pkl')\n",
    "y_train_l = pd.read_pickle('pickles/split/y_train_l.pkl')\n",
    "y_test_l = pd.read_pickle('pickles/split/y_test_l.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_lond_allfeat = XGBRegressor(reg_alpha=0.75, reg_lambda=125, learning_rate=0.3, max_depth=7, n_estimators=70, objective='reg:squarederror', random_state=42)\n",
    "xgb_lond_allfeat.fit(X_train_l,y_train_l)\n",
    "\n",
    "l_allfeat_pred_train = xgb_lond_allfeat.predict(X_train_l)\n",
    "l_allfeat_pred_test = xgb_lond_allfeat.predict(X_test_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_allfeat_rmse_train = root_mean_squared_error(y_train_l,l_allfeat_pred_train)\n",
    "l_allfeat_r2_train = r2_score(y_train_l,l_allfeat_pred_train)\n",
    "\n",
    "l_allfeat_rmse_test = root_mean_squared_error(y_test_l,l_allfeat_pred_test)\n",
    "l_allfeat_r2_test = r2_score(y_test_l,l_allfeat_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: \n",
      "Root Mean Squared Error: 193.01304210896097 \n",
      "R-Squared: 0.9677050709724426\n",
      "Test: \n",
      "Root Mean Squared Error: 299.93461916669133 \n",
      "R-Squared: 0.929388165473938\n"
     ]
    }
   ],
   "source": [
    "print(f'Train: \\nRoot Mean Squared Error: {l_allfeat_rmse_train} \\nR-Squared: {l_allfeat_r2_train}')\n",
    "print(f'Test: \\nRoot Mean Squared Error: {l_allfeat_rmse_test} \\nR-Squared: {l_allfeat_r2_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned with Forward/Backward Select Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DC set, Fw features parameters**:<p>\n",
    "alpha=2, lambda=50, learning_rate=0.2, max_depth=6, n_estimators=370, objective='reg:squarederror', random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dc_fw = pd.read_pickle('pickles/split/feat-select/forward/X_train_dc_fw.pkl')\n",
    "X_test_dc_fw = pd.read_pickle('pickles/split/feat-select/forward/X_test_dc_fw.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_dc_fw = XGBRegressor(reg_alpha=2, reg_lambda=50, learning_rate=0.2, max_depth=6, n_estimators=370, objective='reg:squarederror', random_state=42)\n",
    "xgb_dc_fw.fit(X_train_dc_fw,y_train_dc)\n",
    "\n",
    "dc_fw_pred_train = xgb_dc_fw.predict(X_train_dc_fw)\n",
    "dc_fw_pred_test = xgb_dc_fw.predict(X_test_dc_fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_fw_rmse_train = root_mean_squared_error(y_train_dc,dc_fw_pred_train)\n",
    "dc_fw_r2_train = r2_score(y_train_dc,dc_fw_pred_train)\n",
    "\n",
    "dc_fw_rmse_test = root_mean_squared_error(y_test_dc,dc_fw_pred_test)\n",
    "dc_fw_r2_test = r2_score(y_test_dc,dc_fw_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: \n",
      "Root Mean Squared Error: 26.683375935152764 \n",
      "R-Squared: 0.9745269417762756\n",
      "Test: \n",
      "Root Mean Squared Error: 66.80355713559786 \n",
      "R-Squared: 0.9082472324371338\n"
     ]
    }
   ],
   "source": [
    "print(f'Train: \\nRoot Mean Squared Error: {dc_fw_rmse_train} \\nR-Squared: {dc_fw_r2_train}')\n",
    "print(f'Test: \\nRoot Mean Squared Error: {dc_fw_rmse_test} \\nR-Squared: {dc_fw_r2_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DC set, Bw features parameters**:<p>\n",
    "alpha=0.7, lambda=80, learning_rate=0.2, max_depth=6, n_estimators=280, objective='reg:squarederror', random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dc_bw = pd.read_pickle('pickles/split/feat-select/backward/X_train_dc_bw.pkl')\n",
    "X_test_dc_bw = pd.read_pickle('pickles/split/feat-select/backward/X_test_dc_bw.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_dc_bw = XGBRegressor(reg_alpha=0.7, reg_lambda=80, learning_rate=0.2, max_depth=6, n_estimators=280, objective='reg:squarederror', random_state=42)\n",
    "xgb_dc_bw.fit(X_train_dc_bw,y_train_dc)\n",
    "\n",
    "dc_bw_pred_train = xgb_dc_bw.predict(X_train_dc_bw)\n",
    "dc_bw_pred_test = xgb_dc_bw.predict(X_test_dc_bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_bw_rmse_train = root_mean_squared_error(y_train_dc,dc_bw_pred_train)\n",
    "dc_bw_r2_train = r2_score(y_train_dc,dc_bw_pred_train)\n",
    "\n",
    "dc_bw_rmse_test = root_mean_squared_error(y_test_dc,dc_bw_pred_test)\n",
    "dc_bw_r2_test = r2_score(y_test_dc,dc_bw_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: \n",
      "Root Mean Squared Error: 30.41702162538651 \n",
      "R-Squared: 0.9668996334075928\n",
      "Test: \n",
      "Root Mean Squared Error: 70.3963388464237 \n",
      "R-Squared: 0.8981127142906189\n"
     ]
    }
   ],
   "source": [
    "print(f'Train: \\nRoot Mean Squared Error: {dc_bw_rmse_train} \\nR-Squared: {dc_bw_r2_train}')\n",
    "print(f'Test: \\nRoot Mean Squared Error: {dc_bw_rmse_test} \\nR-Squared: {dc_bw_r2_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**London set, FwBw features parameters**:<p>\n",
    "alpha=0.01, lambda=10, learning_rate=0.1, max_depth=6, n_estimators=150, objective='reg:squarederror', random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_l_fwbw = pd.read_pickle('pickles/split/feat-select/fwbw/X_train_l_bw.pkl')\n",
    "X_test_l_fwbw = pd.read_pickle('pickles/split/feat-select/fwbw/X_test_l_bw.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_lond_fwbw = XGBRegressor(reg_alpha=0.01, reg_lambda=10, learning_rate=0.1, max_depth=6, n_estimators=150, objective='reg:squarederror', random_state=42)\n",
    "xgb_lond_fwbw.fit(X_train_l_fwbw,y_train_l)\n",
    "\n",
    "l_fwbw_pred_train = xgb_lond_fwbw.predict(X_train_l_fwbw)\n",
    "l_fwbw_pred_test = xgb_lond_fwbw.predict(X_test_l_fwbw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_fwbw_rmse_train = root_mean_squared_error(y_train_l,l_fwbw_pred_train)\n",
    "l_fwbw_r2_train = r2_score(y_train_l,l_fwbw_pred_train)\n",
    "\n",
    "l_fwbw_rmse_test = root_mean_squared_error(y_test_l,l_fwbw_pred_test)\n",
    "l_fwbw_r2_test = r2_score(y_test_l,l_fwbw_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: \n",
      "Root Mean Squared Error: 203.8147280096299 \n",
      "R-Squared: 0.9639892578125\n",
      "Test: \n",
      "Root Mean Squared Error: 302.49375523106994 \n",
      "R-Squared: 0.928178071975708\n"
     ]
    }
   ],
   "source": [
    "print(f'Train: \\nRoot Mean Squared Error: {l_fwbw_rmse_train} \\nR-Squared: {l_fwbw_r2_train}')\n",
    "print(f'Test: \\nRoot Mean Squared Error: {l_fwbw_rmse_test} \\nR-Squared: {l_fwbw_r2_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned with Lasso Select Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DC set, Lasso features parameters**:<p>\n",
    "alpha=1, lambda=130, learning_rate=0.2, max_depth=6, n_estimators=190, objective='reg:squarederror', random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dc_lasso = pd.read_pickle('pickles/split/feat-select/lasso/X_train_dc_lasso.pkl')\n",
    "X_test_dc_lasso = pd.read_pickle('pickles/split/feat-select/lasso/X_test_dc_lasso.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_dc_lasso = XGBRegressor(reg_alpha=1, reg_lambda=130, learning_rate=0.2, max_depth=6, n_estimators=190, objective='reg:squarederror', random_state=42)\n",
    "xgb_dc_lasso.fit(X_train_dc_lasso,y_train_dc)\n",
    "\n",
    "dc_lasso_pred_train = xgb_dc_lasso.predict(X_train_dc_lasso)\n",
    "dc_lasso_pred_test = xgb_dc_lasso.predict(X_test_dc_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_lasso_rmse_train = root_mean_squared_error(y_train_dc,dc_lasso_pred_train)\n",
    "dc_lasso_r2_train = r2_score(y_train_dc,dc_lasso_pred_train)\n",
    "\n",
    "dc_lasso_rmse_test = root_mean_squared_error(y_test_dc,dc_lasso_pred_test)\n",
    "dc_lasso_r2_test = r2_score(y_test_dc,dc_lasso_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: \n",
      "Root Mean Squared Error: 35.664152702379106 \n",
      "R-Squared: 0.9544945955276489\n",
      "Test: \n",
      "Root Mean Squared Error: 73.38944653955072 \n",
      "R-Squared: 0.8892644643783569\n"
     ]
    }
   ],
   "source": [
    "print(f'Train: \\nRoot Mean Squared Error: {dc_lasso_rmse_train} \\nR-Squared: {dc_lasso_r2_train}')\n",
    "print(f'Test: \\nRoot Mean Squared Error: {dc_lasso_rmse_test} \\nR-Squared: {dc_lasso_r2_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**London set, Lasso features parameters**:<p>\n",
    "alpha=0.01, lambda=10, learning_rate=0.1, max_depth=6, n_estimators=160, objective='reg:squarederror', random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_l_lasso = pd.read_pickle('pickles/split/feat-select/lasso/X_train_l_lasso.pkl')\n",
    "X_test_l_lasso = pd.read_pickle('pickles/split/feat-select/lasso/X_test_l_lasso.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_lond_lasso = XGBRegressor(reg_alpha=0.01, reg_lambda=10, learning_rate=0.1, max_depth=6, n_estimators=160, objective='reg:squarederror', random_state=42)\n",
    "xgb_lond_lasso.fit(X_train_l_lasso,y_train_l)\n",
    "\n",
    "l_lasso_pred_train = xgb_lond_lasso.predict(X_train_l_lasso)\n",
    "l_lasso_pred_test = xgb_lond_lasso.predict(X_test_l_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_lasso_rmse_train = root_mean_squared_error(y_train_l,l_lasso_pred_train)\n",
    "l_lasso_r2_train = r2_score(y_train_l,l_lasso_pred_train)\n",
    "\n",
    "l_lasso_rmse_test = root_mean_squared_error(y_test_l,l_lasso_pred_test)\n",
    "l_lasso_r2_test = r2_score(y_test_l,l_lasso_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: \n",
      "Root Mean Squared Error: 189.72241781819864 \n",
      "R-Squared: 0.9687969088554382\n",
      "Test: \n",
      "Root Mean Squared Error: 286.0157863908551 \n",
      "R-Squared: 0.9357897639274597\n"
     ]
    }
   ],
   "source": [
    "print(f'Train: \\nRoot Mean Squared Error: {l_lasso_rmse_train} \\nR-Squared: {l_lasso_r2_train}')\n",
    "print(f'Test: \\nRoot Mean Squared Error: {l_lasso_rmse_test} \\nR-Squared: {l_lasso_r2_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparisons<p>\n",
    "Now it is possible to compare all the models' performances. Since all of them use `XGBRegressor`, I am comparing the untuned model to the tuned ones, and also which, if any, feature selection method did the best.<p>\n",
    "First, I'm going to look at the DC dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DC set testing R2 values: \n",
      " Basic model:        0.8994784355163574 \n",
      " Tuned all features: 0.9029864072799683 \n",
      " Fw features:        0.9082472324371338\n",
      " Bw features:        0.8981127142906189 \n",
      " Lasso features:     0.8892644643783569\n"
     ]
    }
   ],
   "source": [
    "print(f\"DC set testing R2 values: \\n Basic model:        0.8994784355163574 \\n Tuned all features: {dc_allfeat_r2_test} \\n Fw features:        {dc_fw_r2_test}\")\n",
    "print(f\" Bw features:        {dc_bw_r2_test} \\n Lasso features:     {dc_lasso_r2_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These R2 values stayed very consistent, regardless of tuning or feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 decreases from training to test, DC set: \n",
      " Basic model:        0.08 \n",
      " Tuned all features: 0.07\n",
      " Fw features:        0.066 \n",
      " Bw features:        0.069\n",
      " Lasso features:     0.065\n"
     ]
    }
   ],
   "source": [
    "print(f\"R2 decreases from training to test, DC set: \\n Basic model:        0.08 \\n Tuned all features: {round(dc_allfeat_r2_train-dc_allfeat_r2_test,3)}\")\n",
    "print(f\" Fw features:        {round(dc_fw_r2_train-dc_fw_r2_test,3)} \\n Bw features:        {round(dc_bw_r2_train-dc_bw_r2_test,3)}\")\n",
    "print(f\" Lasso features:     {round(dc_lasso_r2_train-dc_lasso_r2_test,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in R2 between train and test ranged from 0.06 to 0.08. There was a small improvment for tuned models over the untuned one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DC set RMSE values: \n",
      " Basic model:         69.92294989890084 \n",
      " Tuned all features:  68.69202506023652 \n",
      " Fw features:         66.80355713559786\n",
      " Bw features:         70.3963388464237 \n",
      " Lasso features:      73.38944653955072\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nDC set RMSE values: \\n Basic model:         69.92294989890084 \\n Tuned all features:  {dc_allfeat_rmse_test} \\n Fw features:         {dc_fw_rmse_test}\")\n",
    "print(f\" Bw features:         {dc_bw_rmse_test} \\n Lasso features:      {dc_lasso_rmse_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the RMSE values are very close together, within ~6.6 of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE increase from training to test, DC set: \n",
      " Basic model:        35.832 \n",
      " Tuned all features: 41.454\n",
      " Fw features:        40.12 \n",
      " Bw features:        39.979\n",
      " Lasso features:     37.725\n"
     ]
    }
   ],
   "source": [
    "print(f\"RMSE increase from training to test, DC set: \\n Basic model:        35.832 \\n Tuned all features: {round(dc_allfeat_rmse_test-dc_allfeat_rmse_train,3)}\")\n",
    "print(f\" Fw features:        {round(dc_fw_rmse_test-dc_fw_rmse_train,3)} \\n Bw features:        {round(dc_bw_rmse_test-dc_bw_rmse_train,3)}\")\n",
    "print(f\" Lasso features:     {round(dc_lasso_rmse_test-dc_lasso_rmse_train,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount the error increased by is also very similar for all versions of the model, and comes close to doubling in many cases, but without looking at the full range of values in the data, that doesn't mean much on its own. I will do that later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to repeat those steps for the London dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London set testing R2 values: \n",
      " Basic model:        0.9133748412132263 \n",
      " Tuned all features: 0.929388165473938\n",
      " FwBw features:      0.928178071975708 \n",
      " Lasso features:     0.9357897639274597\n"
     ]
    }
   ],
   "source": [
    "print(f\"London set testing R2 values: \\n Basic model:        0.9133748412132263 \\n Tuned all features: {l_allfeat_r2_test}\")\n",
    "print(f\" FwBw features:      {l_fwbw_r2_test} \\n Lasso features:     {l_lasso_r2_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These R2 values are also very consistent, regardless of tuning or feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 decreases from training to test, London set: \n",
      " Basic model:        0.072 \n",
      " Tuned all features: 0.038\n",
      " FwBw features:      0.036 \n",
      " Lasso features:     0.033\n"
     ]
    }
   ],
   "source": [
    "print(f\"R2 decreases from training to test, London set: \\n Basic model:        0.072 \\n Tuned all features: {round(l_allfeat_r2_train-l_allfeat_r2_test,3)}\")\n",
    "print(f\" FwBw features:      {round(l_fwbw_r2_train-l_fwbw_r2_test,3)} \\n Lasso features:     {round(l_lasso_r2_train-l_lasso_r2_test,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in R2 between the train and test sets is much smaller for tuned models on this dataset. That was still the case on the DC dataset, but the difference was smaller. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London set testing RMSE values: \n",
      " Basic model:        332.20786503370334 \n",
      " Tuned all features: 299.93461916669133\n",
      " FwBw features:      302.49375523106994 \n",
      " Lasso features:     286.0157863908551\n"
     ]
    }
   ],
   "source": [
    "print(f\"London set testing RMSE values: \\n Basic model:        332.20786503370334 \\n Tuned all features: {l_allfeat_rmse_test}\")\n",
    "print(f\" FwBw features:      {l_fwbw_rmse_test} \\n Lasso features:     {l_lasso_rmse_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE increase from training to test, London set: \n",
      " Basic model:        203.221 \n",
      " Tuned all features: 106.922\n",
      " FwBw features:       98.679 \n",
      " Lasso features:      96.293\n"
     ]
    }
   ],
   "source": [
    "print(f\"RMSE increase from training to test, London set: \\n Basic model:        203.221 \\n Tuned all features: {round(l_allfeat_rmse_test-l_allfeat_rmse_train,3)}\")\n",
    "print(f\" FwBw features:       {round(l_fwbw_rmse_test-l_fwbw_rmse_train,3)} \\n Lasso features:      {round(l_lasso_rmse_test-l_lasso_rmse_train,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time there is a lot of difference in the amount the error increased between the training and test sets; the tuned models have a much smaller increase, and the feature selected versions are slightly smaller still. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With both sets, the difference between the three tuned models was very slight, and the difference between those and the basic model was still slight.<p>\n",
    "Both models have a high R2 score, so it seems that the models do a good job of accounting for the data. To get an idea of if the RMSE is in a decent range or not, we need to know the maximum values in each set. (The minimum is 0.) Looking at the RMSE makes it seem like `XGBRegressor` is better able to handle the London dataset, but given that data has higher count values, that might not actually be true. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_data = pd.read_pickle('pickles/dc_data.pkl')\n",
    "london_data = pd.read_pickle('pickles/london_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DC count max: 977\n",
      "London count max: 7860\n"
     ]
    }
   ],
   "source": [
    "print('DC count max:',dc_data['count'].max())\n",
    "print('London count max:',london_data['count'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see, proportionally, how much the error there was, and how much the error was increasing from training to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_max = 977 \n",
    "london_max = 7860"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DC set, RMSE as percentage of range: \n",
      "Basic model:        7.16 \n",
      "Tuned all features: 7.03\n",
      "Fw features:        6.84 \n",
      "Bw features:        7.21 \n",
      "Lasso features:     7.51\n",
      "\n",
      "DC set, RMSE increase as percent of range: \n",
      "Basic model:        3.67 \n",
      "Tuned all features: 4.24\n",
      "Fw features:        4.11 \n",
      "Bw features:        4.09\n",
      "Lasso features:     3.86\n"
     ]
    }
   ],
   "source": [
    "print(f\"DC set, RMSE as percentage of range: \\nBasic model:        {round(69.92294989890084/dc_max*100,2)} \\nTuned all features: {round(dc_allfeat_rmse_test/dc_max*100,2)}\")\n",
    "print(f\"Fw features:        {round(dc_fw_rmse_test/dc_max*100,2)} \\nBw features:        {round(dc_bw_rmse_test/dc_max*100,2)} \\nLasso features:     {round(dc_lasso_rmse_test/dc_max*100,2)}\")\n",
    "print(f\"\\nDC set, RMSE increase as percent of range: \\nBasic model:        {round(35.832/dc_max*100,2)} \\nTuned all features: {round((dc_allfeat_rmse_test-dc_allfeat_rmse_train)/dc_max*100,2)}\")\n",
    "print(f\"Fw features:        {round((dc_fw_rmse_test-dc_fw_rmse_train)/dc_max*100,2)} \\nBw features:        {round((dc_bw_rmse_test-dc_bw_rmse_train)/dc_max*100,2)}\") \n",
    "print(f\"Lasso features:     {round((dc_lasso_rmse_test-dc_lasso_rmse_train)/dc_max*100,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London set, RMSE as percentage of range: \n",
      "Basic model:        4.23 \n",
      "Tuned all features: 3.82\n",
      "FwBw features:      3.85 \n",
      "Lasso features:     3.64\n",
      "\n",
      "London set, RMSE increase as percent of range: \n",
      "Basic model:        2.59\n",
      "Tuned all features: 1.36 \n",
      "FwBw features:      1.26\n",
      "Lasso features:     1.23\n"
     ]
    }
   ],
   "source": [
    "print(f\"London set, RMSE as percentage of range: \\nBasic model:        {round(332.20786503370334/london_max*100,2)} \\nTuned all features: {round(l_allfeat_rmse_test/london_max*100,2)}\")\n",
    "print(f\"FwBw features:      {round(l_fwbw_rmse_test/london_max*100,2)} \\nLasso features:     {round(l_lasso_rmse_test/london_max*100,2)}\")\n",
    "print(f\"\\nLondon set, RMSE increase as percent of range: \\nBasic model:        {round(203.221/london_max*100,2)}\")\n",
    "print(f\"Tuned all features: {round((l_allfeat_rmse_test-l_allfeat_rmse_train)/london_max*100,2)} \\nFwBw features:      {round((l_fwbw_rmse_test-l_fwbw_rmse_train)/london_max*100,2)}\")\n",
    "print(f\"Lasso features:     {round((l_lasso_rmse_test-l_lasso_rmse_train)/london_max*100,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, if we compare the RMSE to the count standard deviation for each set - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DC's count's std:  181.5 \n",
      "DC'S worst RMSE: ~73\n",
      "\n",
      "London's count's std: 1085.4 \n",
      "London's worst RMSE: ~332\n"
     ]
    }
   ],
   "source": [
    "print(f\"DC's count's std:  {round(dc_data['count'].std(),1)} \\nDC'S worst RMSE: ~73\")\n",
    "print(f\"\\nLondon's count's std: {round(london_data['count'].std(),1)} \\nLondon's worst RMSE: ~332\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, the RMSE is significantly lower than the standard deviation, so the model is accounting for a good portion of the variation occuring.<p>\n",
    "Looking at the error from this perspective, it's clear that the model is in fact doing quite well at accounting for the variations in the datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
