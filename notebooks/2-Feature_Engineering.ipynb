{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Feature Engineering<p>\n",
    "Import libraries and first dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_hour = pd.read_csv('../dc-data/hour.csv')\n",
    "dc_day = pd.read_csv('../dc-data/day.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -> DC Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the missing hour values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feat_eng_functions import fill_hours_dc\n",
    "dc_filled = fill_hours_dc(dc_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling values that are identical within each day:<br>\n",
    "(Trying to do this in a function resulted in unexpected NaNs, so I am doing it like this.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_filled.loc[:,'season'] = dc_filled.groupby('dteday')['season'].transform(lambda x: x.fillna(x.mean().astype(int)))\n",
    "dc_filled.loc[:, 'yr'] = dc_filled.groupby('dteday')['yr'].transform(lambda x: x.fillna(x.mean().astype(int)))\n",
    "dc_filled.loc[:, 'mnth'] = dc_filled.groupby('dteday')['mnth'].transform(lambda x: x.fillna(x.mean().astype(int)))\n",
    "dc_filled.loc[:, 'holiday'] = dc_filled.groupby('dteday')['holiday'].transform(lambda x: x.fillna(x.mean().astype(int)))\n",
    "dc_filled.loc[:, 'weekday'] = dc_filled.groupby('dteday')['weekday'].transform(lambda x: x.fillna(x.mean().astype(int)))\n",
    "dc_filled.loc[:, 'workingday'] = dc_filled.groupby('dteday')['workingday'].transform(lambda x: x.fillna(x.mean().astype(int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using interpolate to fill values that move throughout a day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_filled.loc[:,'temp'] = dc_filled['temp'].interpolate(method='linear', limit_direction='both').round(2)\n",
    "dc_filled.loc[:,'atemp'] = dc_filled['atemp'].interpolate(method='linear', limit_direction='both').round(4)\n",
    "dc_filled.loc[:,'hum'] = dc_filled['hum'].interpolate(method='linear', limit_direction='both').round(2)\n",
    "dc_filled.loc[:,'windspeed'] = dc_filled['windspeed'].interpolate(method='linear', limit_direction='both').round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `dc_day` table's `weathersit` column as an 'overall' weather value for that day to fill missing weather values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_day['dteday'] = pd.to_datetime(dc_day['dteday'])\n",
    "dc_filled = dc_filled.merge(dc_day[['dteday','weathersit']],on='dteday',how='left')\n",
    "dc_filled['weathersit_x'] = dc_filled['weathersit_x'].fillna(dc_filled['weathersit_y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a categorical `precip` colunmn for whether or not there was precipitation, based on documentation's description of `weathersit` values;<p>\n",
    "**0 (no precipitation):**<ul>\n",
    "- 1: Clear, Few clouds, Partly cloudy, Partly cloudy<br>\n",
    "- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist</ul>\n",
    "\n",
    "**1 (precipitation):**<ul>\n",
    "- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds<br>\n",
    "- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_map = {1:0,2:0,3:1,4:1}\n",
    "dc_filled['precip'] = dc_filled['weathersit_x'].map(precip_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a column for the numerical day of the month from the `dteday` column, since I have been advised I don't have a long enough interval to do a proper timeseries analysis, and I have the other elements of the date in their own columns anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feat_eng_functions import get_day\n",
    "dc_filled['day'] = dc_filled['dteday'].apply(lambda x: get_day(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up table; turn floats back to int, rename columns, drop extra `weathersit` column. Also drop `casual` and `registered`, as they are redundant to `cnt` and not needed.<br>\n",
    "_(Doing this via column select so I can reorder the columns to my liking.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_data = dc_filled[['dteday','season','yr','mnth','day','hr','weekday',\n",
    "    'holiday','workingday','weathersit_x','precip','temp','atemp','hum','windspeed','cnt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_data[['season','yr','mnth','holiday','weekday','workingday','weathersit_x','cnt']] = dc_data[['season','yr','mnth','holiday','weekday','workingday','weathersit_x','cnt']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_data = dc_data.rename(columns={'dteday':'date','yr':'year','mnth':'month','hr':'hour',\n",
    "'weathersit_x':'weather','hum':'humidity','cnt':'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -> London Dataset<p>\n",
    "Import second dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "london = pd.read_csv('../london-data/london_merged.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match `season` column to DC dataset:<ul>\n",
    "0 -> 1 (spring)<br>\n",
    "1 -> 2 (summer)<br>\n",
    "2 -> 3 (fall)<br>\n",
    "3 -> 4 (winter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seas_map = {0:1,1:2,2:3,3:4}\n",
    "london['season'] = london['season'].map(seas_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match `weather_code` to DC dataset:<p>\n",
    "\n",
    "**1: Clear, Few clouds, Partly cloudy, Partly cloudy**<br>\n",
    "Mapping in:<ul>\n",
    "1 = Clear ; mostly clear but have some values with haze/fog/patches of fog/ fog in vicinity<br>\n",
    "2 = scattered clouds / few clouds </ul>\n",
    "\n",
    "**2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist**<br>\n",
    "Mapping in:<ul>\n",
    "3 = Broken clouds<br>\n",
    "4 = Cloudy</ul>\n",
    "\n",
    "**3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds**<br>\n",
    "Mapping in:<ul>\n",
    "7 = Rain/ light Rain shower/ Light rain</ul>\n",
    "\n",
    "**4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog**<br>\n",
    "Mapping in:<ul>\n",
    "10 = rain with thunderstorm<br>\n",
    "26 = snowfall<br>\n",
    "94 = Freezing Fog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_map = {1:1,2:1,3:2,4:2,7:3,10:4,26:4,94:4}\n",
    "london['weather_code'] = london['weather_code'].map(weather_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `workingday` column using `is_holiday` and `is_weekend` columns; any day that has a **1** in either column is not a working day and gets a 0 in the new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column with default value 1\n",
    "london['workingday'] = 1\n",
    "\n",
    "# assign 0 in appropriate rows\n",
    "london.loc[((london[(london['is_holiday']==1.0)| # is a holiday OR\n",
    "                    (london['is_weekend']==1.0)  # is a weekend \n",
    "                    ].index).tolist()), 'workingday'] = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get `hours` column out of `timestamp`, and then check for missing hour rows. (The prior function was specific to the column names of the DC dataset, so another one is necessary.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feat_eng_functions import get_hour\n",
    "\n",
    "london['timestamp'] = pd.to_datetime(london['timestamp'])\n",
    "\n",
    "london['hour'] = london['timestamp'].apply(lambda x: get_hour(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feat_eng_functions import fill_hours_london\n",
    "london_filled = fill_hours_london(london)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the rest of the time-related values out of the `timestamp` column; `year`, `month`, `day` and `weekday`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feat_eng_functions import get_year, get_month, get_weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_filled['year'] = london_filled['timestamp'].apply(lambda x: get_year(x))\n",
    "london_filled['month'] = london_filled['timestamp'].apply(lambda x: get_month(x))\n",
    "london_filled['day'] = london_filled['timestamp'].apply(lambda x: get_day(x))\n",
    "london_filled['weekday'] = london_filled['timestamp'].apply(lambda x: get_weekday(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill values that are uniform for each day using `groupby` and `mean`, same as the DC dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_filled.loc[:, 'season'] = london_filled.groupby(['year','month','day'])['season'].transform(lambda x: x.fillna(x.mean()))\n",
    "london_filled.loc[:, 'workingday'] = london_filled.groupby(['year','month','day'])['workingday'].transform(lambda x: x.fillna(x.mean()))\n",
    "london_filled.loc[:, 'is_holiday'] = london_filled.groupby(['year','month','day'])['is_holiday'].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any rows that still have NaNs after this step are missing all of their data, and need to be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_filled = london_filled.dropna(subset=['season','workingday','is_holiday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "season  workingday  is_holiday\n",
       "False   False       False         17520\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "london_filled[['season','workingday','is_holiday']].isna().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill NaNs using interpolate for values that move throughout the day, same as the DC dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_filled.loc[:, 't1'] = london_filled['t1'].interpolate(method='linear', limit_direction='both').round(1)\n",
    "london_filled.loc[:, 't2'] = london_filled['t2'].interpolate(method='linear', limit_direction='both').round(1)\n",
    "london_filled.loc[:, 'hum'] = london_filled['hum'].interpolate(method='linear', limit_direction='both').round(1)\n",
    "london_filled.loc[:, 'wind_speed'] = london_filled['wind_speed'].interpolate(method='linear', limit_direction='both').round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the DC data, where there was a second table with the weather codes for the day to use to fill in NaNs in the weather column, I am going to use forward fill for the missing weather values in this case. Looking at several of the NaNs showed that unlike in the DC data, they were usually flanked on either side by the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_filled.loc[:, 'weather_code'] = london_filled['weather_code'].ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the `precip` column, using the same mapping as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_filled.loc[:,'precip'] = london_filled['weather_code'].map(precip_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DC data has a different mapping for days of the week than the default one created by extracting the days of the week from a datetime object, so for consistency I am remapping this dataset to match the DC dataset.<ul>\n",
    "6 -> 0 (Sunday)<br>\n",
    "0 -> 1 (Monday)<br>\n",
    "1 -> 2 (Tuesday)<br>\n",
    "2 -> 3 (Wednesday)<br>\n",
    "3 -> 4 (Thursday)<br>\n",
    "4 -> 5 (Friday)<br>\n",
    "5 -> 6 (Saturday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_map = {6:0,0:1,1:2,2:3,3:4,4:5,5:6}\n",
    "london_filled.loc[:,'weekday'] = london_filled['weekday'].map(day_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has two nearly complete years and a small part of a third; the mapping for those is as follows:<ul>\n",
    "2015 -> 0<br>\n",
    "2016 -> 1<br>\n",
    "2017 -> 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_map = {2015:0,2016:1,2017:2}\n",
    "london_filled.loc[:,'year'] = london_filled['year'].map(year_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't want the `is_weekend` column, so I will not be including that, and I am going to reorder my columns, and rename them so they are 1:1 with the other dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_data = london_filled[['timestamp','season','year','month','day','hour','weekday','is_holiday',\n",
    "    'workingday','weather_code','precip','t1', 't2', 'hum', 'wind_speed','cnt',]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_data = london_data.rename(columns={'cnt':'count','t1':'temp','t2':'atemp','hum':'humidity',\n",
    "'wind_speed':'windspeed','weather_code':'weather','is_holiday':'holiday',\n",
    "'timestamp':'date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_data[['season','holiday','workingday','weather','count']] = london_data[['season','holiday','workingday','weather','count']].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, I need to normalize the `temp`, `atemp`, `humidity`, and `windspeed` columns, as they came pre-normalized in the DC dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(copy=False)\n",
    "london_data[['temp','atemp','humidity','windspeed']] = scaler.fit_transform(london_data[['temp','atemp','humidity','windspeed']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting `date` columns as the index of the table, since they are redundant with the columns I have extracted from them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_data = dc_data.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_data = london_data.set_index('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save pickles of processed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_data.to_pickle('pickles/dc_data.pkl')\n",
    "london_data.to_pickle('pickles/london_data.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
